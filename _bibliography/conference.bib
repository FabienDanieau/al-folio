---
---

@inproceedings{danieau2019automatic,
  title={Automatic Generation and Stylization of 3D Facial Rigs},
  author={Danieau, Fabien and Gubins, IIja and Olivier, Nicolas and Dumas, Olivier and Denis, Bernard and Lopez, Thomas and Mollet, Nicolas and Frager, Brian and Avril, Quentin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
  pages={784--792},
  year={2019},
  organization={IEEE},
  doi = {10.1109/VR.2019.8798208},
  abstract = {In this paper, we present a fully automatic pipeline for generating and stylizing high geometric and textural quality facial rigs. They are automatically rigged with facial blendshapes for animation, and can be used across platforms for applications including virtual reality, augmented reality, remote collaboration, gaming and more. From a set of input facial photos, our approach is to be able to create a photorealistic, fully rigged character in less than seven minutes. The facial mesh reconstruction is based on state-of-the art photogrammetry approaches. Automatic landmarking coupled with ICP registration with regularization provide direct correspondence and registration from a given generic mesh to the acquired facial mesh. Then, using deformation transfer, existing blendshapes are transferred from the generic to the reconstructed facial mesh. The reconstructed face is then fit to the full body generic mesh. Extra geometry such as jaws, teeth and nostrils are retargeted and transferred to the character. An automatic iris color extraction algorithm is performed to colorize a separate eye texture, animated with dynamic UVs. Finally, an extra step applies a style to the photorealis-tic face to enable blending of personalized facial features into any other character. The user's face can then be adapted to any human or non-human generic mesh. A pilot user study was performed to evaluate the utility of our approach. Up to 65% of the participants were successfully able to discern the presence of one's unique facial features when the style was not too far from a humanoid shape.},
  pdf = {fdanieau.free.fr/pubs/ieeevr2019.pdf}
}

@inproceedings{danieau2018hfx,
  title={HFX studio: haptic editor for full-body immersive experiences},
  author={Danieau, Fabien and Guillotel, Philippe and Dumas, Olivier and Lopez, Thomas and Leroy, Bertrand and Mollet, Nicolas},
  booktitle={Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
  pages={1--9},
  year={2018},
  doi = {10.1145/3281505.3281518},
  abstract = {Current virtual reality systems enable users to explore virtual worlds, fully embodied in avatars. This new type of immersive experience requires specific authoring tools. The traditional ones used in the movie and the video games industries were modified to support immersive visual and audio content. However, few solutions exist to edit haptic content, especially when the whole user's body is involved. To tackle this issue we propose HFX Studio, a haptic editor based on haptic perceptual models. Three models of pressure, vibration and temperature were defined to allow the spatialization of haptic effects on the user's body. These effects can be designed directly on the body (egocentric approach), or specified as objects of the scene (allocentric approach). The perceptual models are also used to describe capabilities of haptic devices. This way the created content is generic, and haptic feedback is rendered on the available devices. The concept has been implemented with the Unity®game engine, a tool already used in VR production. A qualitative pilot user study was conducted to analyze the usability of our tool with expert users. Results shows that the edition of haptic feedback is intuitive for these users.},
  pdf = {fdanieau.free.fr/pubs/hapticEditor.pdf}
}

@inproceedings{costes2018kinestouch,
  title={KinesTouch: 3D Force-Feedback Rendering for Tactile Surfaces},
  author={Costes, Antoine and Danieau, Fabien and Argelaguet-Sanz, Ferran and L{\'e}cuyer, Anatole and Guillotel, Philippe},
  booktitle={International Conference on Virtual Reality and Augmented Reality},
  pages={97--116},
  year={2018},
  organization={Springer},
  abstract = {In this paper, we introduce the KinesTouch, a novel approach for tactile screen enhancement providing four types of haptic feedback with a single force-feedback device: compliance, friction, fine roughness, and shape. We present the design and implementation of a corresponding set of haptic effects as well as a proof-of-concept setup. Regarding friction in particular, we propose a novel effect based on large lateral motion that increases or diminishes the sliding velocity between the finger and the screen. A user study was conducted on this effect to confirm its ability to produce distinct sliding sensations. Visual cues were confirmed to influence sliding judgments, but further studies would help clarifying the role of tactile cues. Finally, we showcase several use cases illustrating the possibilities offered by the KinesTouch to enhance 2D and 3D interactions on tactile screens in various contexts.},
  doi = {doi.org/10.1007/978-3-030-01790-3_6},
  pdf = {people.irisa.fr/Antoine.Costes/assets/publications/kinestouch.pdf}
}

@inproceedings{callens2018tangible,
  title={A Tangible Surface for Digital Sculpting in Virtual Environments},
  author={Callens, Edouard and Danieau, Fabien and Costes, Antoine and Guillotel, Philippe},
  booktitle={International Conference on Human Haptic Sensing and Touch Enabled Computer Applications},
  pages={157--168},
  year={2018},
  organization={Springer},
  abstract = {With the growth of virtual reality setups, digital sculpting tools become more and more immersive. It is now possible to create a piece of art within a virtual environment, directly with the controllers. However, these devices do not allow to touch the virtual material as a sculptor would do. To tackle this issue we investigate in this paper the use of a tangible surface that could be used in virtual reality setups. We designed a low-cost prototype composed of two layers of sensors in order to measure a wide range of pressure. We also propose two mapping techniques to fit our device to a virtual 3D mesh to be sculpted. Participants of an informal test were asked to reproduce a pattern on three meshes: a plane, a sphere and a teapot. They succeeded in this task, showing the potential of our approach.},
  doi = {10.1007/978-3-319-93399-3_15},
  pdf = {fdanieau.free.fr/pubs/tangibleCreation.pdf}
}

@inproceedings{costes2018haptic,
  title={Haptic material: A holistic approach for haptic texture mapping},
  author={Costes, Antoine and Danieau, Fabien and Argelaguet, Ferran and L{\'e}cuyer, Anatole and Guillotel, Philippe},
  booktitle={International Conference on Human Haptic Sensing and Touch Enabled Computer Applications},
  pages={37--45},
  year={2018},
  organization={Springer},
  abstract = {In this paper, we propose a new format for haptic texture mapping which is not dependent on the haptic rendering setup hardware. Our “haptic material” format encodes ten elementary haptic features in dedicated maps, similarly to “materials” used in computer graphics. These ten different features enable the expression of compliance, surface geometry and friction attributes through vibratory, cutaneous and kinesthetic cues, as well as thermal rendering. The diversity of haptic data allows various hardware to share this single format, each of them selecting which features to render depending on its capabilities.},
  doi = {10.1007/978-3-319-93399-3_4},
  pdf = {fdanieau.free.fr/pubs/Eurohaptics_2018_HapticMaterial.pdf}
}

@inproceedings{danieau2017enabling,
  title={Enabling embodiment and interaction in omnidirectional videos},
  author={Danieau, Fabien and Lopez, Thomas and Mollet, Nicolas and Leroy, Bertrand and Dumas, Olivier and Vial, Jean-Francois},
  booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={697--702},
  year={2017},
  organization={IEEE},
  doi = {10.1109/ICME.2017.8019388},
  abstract = {This paper investigates the role of the embodiment in an immersive video experience. A system allowing to play back omnidirectional videos enhanced with real-time 3D content is presented. It enables the user to be embodied in an avatar and to interact with 3D objects added to the video. A user study was conducted to understand the impact of this embodiment on the user experience. Four different avatars were compared (none, human, ghost and astronaut) as well as three levels of user control on the avatar, and also the possibility to interact with the content. Results show that being embodied has benefits on the user experience but requires the representation to be in line with the story and the avatar to be fully controllable. Indeed a misadapted embodiment may decrease the experience.},
  pdf = {fdanieau.free.fr/pubs/embodiment.pdf}
}

@inproceedings{danieau2017attention,
  title={Attention guidance for immersive video content in head-mounted displays},
  author={Danieau, Fabien and Guillo, Antoine and Dor{\'e}, Renaud},
  booktitle={2017 IEEE Virtual Reality (VR)},
  pages={205--206},
  year={2017},
  organization={IEEE},
  doi={10.1109/VR.2017.7892248},
  abstract={Immersive videos allow users to freely explore 4 π steradian scenes within head-mounted displays (HMD), leading to a strong feeling of immersion. However users may miss important elements of the narrative if not facing them. Hence, we propose four visual effects to guide the user's attention. After an informal pilot study, two of the most efficient effects were evaluated through a user study. Results show that our approach has potential but it remains challenging to implicitly drive the user's attention outside of the field of view.},
  pdf={fdanieau.free.fr/pubs/gazeincitement.pdf}
}

@inproceedings{danieau2012hapseat,
  title={HapSeat: producing motion sensation with multiple force-feedback devices embedded in a seat},
  author={Danieau, Fabien and Fleureau, Julien and Guillotel, Philippe and Mollet, Nicolas and L{\'e}cuyer, Anatole and Christie, Marc},
  booktitle={Proceedings of the 18th ACM symposium on Virtual reality software and technology},
  pages={69--76},
  year={2012},
  doi={10.1145/2407336.2407350},
  pdf={hal.inria.fr/docs/00/76/62/58/PDF/100-danieau.pdf},
  abstract={We introduce a novel way of simulating sensations of motion which does not require an expensive and cumbersome motion platform. Multiple force-feedbacks are applied to the seated user's body to generate a sensation of motion experiencing passive navigation. A set of force-feedback devices such as mobile armrests or headrests are arranged around a seat so that they can apply forces to the user. We have dubbed this new approach HapSeat. A proof of concept has been designed which uses three low-cost force-feedback devices, and two control models have been implemented. Results from the first user study suggest that subjective sensations of motion are reliably generated using either model. Our results pave the way to a novel device to generate consumer motion effects based on our prototype.}
}

@inproceedings{fleureau2016texture,
  title={Texture rendering on a tactile surface using extended elastic images and example-based audio cues},
  author={Fleureau, Julien and Lefevre, Yoan and Danieau, Fabien and Guillotel, Philippe and Costes, Antoine},
  booktitle={International Conference on Human Haptic Sensing and Touch Enabled Computer Applications},
  pages={350--359},
  year={2016},
  organization={Springer},
  doi={10.1007/978-3-319-42321-0_32},
  pdf={fdanieau.free.fr/pubs/eurohaptics.pdf},
  abstract={A texture rendering system relying on pseudo-haptic and audio feedback is presented in this paper. While the user touches the texture displayed on a tactile screen, the associated image is deformed according to the contact area and the rubbing motion to simulate pressure. Additionally audio feedback is synthesized in real-time to simulate friction. A novel example-based scheme takes advantage of recorded audio samples of friction between actual textures and a finger at several speeds to synthesize the final output sound. This system can be implemented on any existing tactile screen without any extra mechanical device.}
}

@inproceedings{rouanet2011robotic,
  title={A robotic game to evaluate interfaces used to show and teach visual objects to a robot in real world condition},
  author={Rouanet, Pierre and Danieau, Fabien and Oudeyer, Pierre-Yves},
  booktitle={Proceedings of the 6th international conference on Human-robot interaction},
  pages={313--320},
  pdf={flowers.inria.fr/Rouanet-Danieau-Oudeyer-hri2011.pdf},
  year={2011},
  doi={10.1145/1957656.1957782},
  abstract={In this paper, we present a real world user study of 4 interfaces designed to teach new visual objects to a social robot. This study was designed as a robotic game in order to maintain the user's motivation during the whole experiment. Among the 4 interfaces 3 were based on mediator objects such as an iPhone, a Wiimote and a laser pointer. They also provided the users with different kind of feedback of what the robot is perceiving. The fourth interface was a gesture based interface with a Wizard-of-Oz recognition system added to compare our mediator interfaces with a more natural interaction. Here, we specially studied the impact the interfaces have on the quality of the learning examples and the usability. We showed that providing non-expert users with a feedback of what the robot is perceiving is needed if one is interested in robust interaction. In particular, the iPhone interface allowed non-expert users to provide better learning examples due to its whole visual feedback. Furthermore, we also studied the user's gaming experience and found that in spite of its lower usability, the gestures interface was stated as entertaining as the other interfaces and increases the user's feeling of cooperating with the robot. Thus, we argue that this kind of interface could be well-suited for robotic game.}
}

@inproceedings{danieau2012framework,
  title={Framework for enhancing video viewing experience with haptic effects of motion},
  author={Danieau, Fabien and Fleureau, Julien and Cabec, Audrey and Kerbiriou, Paul and Guillotel, Philippe and Mollet, Nicolas and Christie, Marc and L{\'e}cuyer, Anatole},
  booktitle={2012 IEEE Haptics Symposium (HAPTICS)},
  pages={541--546},
  year={2012},
  pdf={hal.inria.fr/docs/00/76/62/57/PDF/Danieau2012-_A_framework_for_enhancing_video_viewing_experience_with_haptic_effects_of_motion.pdf},
  organization={IEEE},
  doi={10.1109/HAPTIC.2012.6183844},
  abstract={This work aims at enhancing a classical video viewing experience by introducing realistic haptic feelings in a consumer environment. More precisely, a complete framework to both produce and render the motion embedded in an audiovisual content is proposed to enhance a natural movie viewing session. We especially consider the case of a first-person point of view audiovisual content and we propose a general workflow to address this problem. This latter includes a novel approach to both capture the motion and video of the scene of interest, together with a haptic rendering system for generating a sensation of motion. A complete methodology to evaluate the relevance of our framework is finally proposed and demonstrates the interest of our approach.}
}
